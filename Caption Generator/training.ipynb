{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_doc(filename):\r\n",
    "\r\n",
    "    # loading the file and returing the data of file \r\n",
    "\r\n",
    "    file = open(filename,'r')\r\n",
    "    text = file.read()\r\n",
    "    text = text.split(\"\\n\")\r\n",
    "    file.close()\r\n",
    "    return text "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text = load_doc(\"./Flickr8k_text/Flickr8k.token.txt\")\r\n",
    "text[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
       " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .',\n",
       " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .',\n",
       " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def caption_grouping(captions):\r\n",
    "\r\n",
    "    # returns a dictionary \r\n",
    "    # key contains image name \r\n",
    "    # values contains 5 captions of the img respectively \r\n",
    " \r\n",
    "    img_captions = dict()\r\n",
    "    captions = captions[:-1]\r\n",
    "    temp = []\r\n",
    "    for cap in captions:\r\n",
    "        if len(cap)<20:\r\n",
    "            continue\r\n",
    "        else:\r\n",
    "            temp_key = cap.split('#')\r\n",
    "            key = temp_key[0]\r\n",
    "            temp_caption = temp_key[-1].split('\\t')\r\n",
    "            if temp_caption[0]=='4':\r\n",
    "                temp.append(temp_caption[-1])\r\n",
    "                img_captions[key]=temp\r\n",
    "                temp=[]\r\n",
    "            else:\r\n",
    "                temp.append(temp_caption[-1])\r\n",
    "    return img_captions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "img_captions = caption_grouping(text)\r\n",
    "list(img_captions.items())[:2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('1000268201_693b08cb0e.jpg',\n",
       "  ['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       "   'A girl going into a wooden building .',\n",
       "   'A little girl climbing into a wooden playhouse .',\n",
       "   'A little girl climbing the stairs to her playhouse .',\n",
       "   'A little girl in a pink dress going into a wooden cabin .']),\n",
       " ('1001773457_577c3a7d70.jpg',\n",
       "  ['A black dog and a spotted dog are fighting',\n",
       "   'A black dog and a tri-colored dog playing with each other on the road .',\n",
       "   'A black dog and a white dog with brown spots are staring at each other in the street .',\n",
       "   'Two dogs of different breeds looking at each other on the road .',\n",
       "   'Two dogs on pavement moving toward each other .'])]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import re "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def caption_preprocessing(img_captions):\r\n",
    "\r\n",
    "    # caption in the dictionary are processed and updated dictionay is returned \r\n",
    "\r\n",
    "    maxlen = 0\r\n",
    "    for key,val in img_captions.items():\r\n",
    "        temp=[]\r\n",
    "        for sent in val:\r\n",
    "            prepro = sent.lower()\r\n",
    "            prepro = re.sub('[^a-z0-9 ]','',prepro)\r\n",
    "            prepro = [ word for word in prepro.split() if len(word)>1 ]\r\n",
    "            prepro = [ word for word in prepro if (word.isalpha())]\r\n",
    "            if len(prepro)>maxlen:\r\n",
    "                maxlen = len(prepro)\r\n",
    "            prepro = ' '.join(prepro)\r\n",
    "            temp.append(prepro)\r\n",
    "            \r\n",
    "        img_captions[key] = temp\r\n",
    "    return (img_captions,maxlen)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "description , max_length= caption_preprocessing(img_captions)\r\n",
    "max_length\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def text_vocabulary(description):\r\n",
    "\r\n",
    "    # returns the vocabulary of captions of all images \r\n",
    "\r\n",
    "    vocab = set()\r\n",
    "    for key in description.keys():\r\n",
    "        for d in description[key]:\r\n",
    "            vocab.update(d.split())\r\n",
    "    return vocab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "vocab = text_vocabulary(description)\r\n",
    "vocab_size = len(vocab)\r\n",
    "vocab_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8763"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from pickle import dump, load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def save_description(description,filename):\r\n",
    "\r\n",
    "    # saves the processed caption in a txt document \r\n",
    "\r\n",
    "    lines=list()\r\n",
    "    for key , val in description.items():\r\n",
    "        for cap in val:\r\n",
    "            lines.append(key+\"\\t\"+cap)\r\n",
    "    data = '\\n'.join(lines)\r\n",
    "    file = open(\"./\"+filename,\"w\")\r\n",
    "    file.write(data)\r\n",
    "    file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "save_description(description,\"caption_description.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from tensorflow.keras.applications.xception import Xception , preprocess_input\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm \r\n",
    "from pickle import dump , load"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "img_dataset_dir = \"./Flickr8k_Dataset/Flicker8k_Dataset\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def extract_features(directory):\r\n",
    "    model = Xception(include_top=False, pooling = 'avg')\r\n",
    "    features = dict()\r\n",
    "    for img_name in tqdm(os.listdir(directory)):\r\n",
    "        img = Image.open(directory+'/'+img_name)\r\n",
    "        img = img.resize((299,299))\r\n",
    "        img = np.expand_dims(img, axis=0)\r\n",
    "        img = img/127.5\r\n",
    "        img = img-1\r\n",
    "\r\n",
    "        feature = model.predict(img)\r\n",
    "        features[img_name]=feature\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "features = extract_features(img_dataset_dir)\r\n",
    "dump(features,open(\"features.p\",'wb'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 8091/8091 [15:45<00:00,  8.55it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "features = load(open(\"features.p\",\"rb\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def load_photos(filename):\r\n",
    "    file = load_doc(filename)\r\n",
    "    photos = file[:-1]\r\n",
    "    return photos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "train_images_path = \"./Flickr8k_text/Flickr_8k.trainImages.txt\"\r\n",
    "description_path = \"./caption_description.txt\"\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\r\n",
    "train_images = load_photos(train_images_path)\r\n",
    "train_images[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['2513260012_03d33305cf.jpg',\n",
       " '2903617548_d3e38d7f88.jpg',\n",
       " '3338291921_fe7ae0c8f8.jpg',\n",
       " '488416045_1c6d903fe0.jpg',\n",
       " '2644326817_8f45080b87.jpg']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def load_cleaned_descriptions(path,photos):\r\n",
    "    file = open(path,'r')\r\n",
    "    text = file.read()\r\n",
    "    file.close()\r\n",
    "    text = text.split(\"\\n\")\r\n",
    "    descriptions={}\r\n",
    "    for sent in text:\r\n",
    "        sent = sent.split('\\t')\r\n",
    "        img = sent[0]\r\n",
    "        if img in photos:\r\n",
    "            if img not in descriptions:\r\n",
    "                descriptions[img] = []\r\n",
    "            des = \"<start> \"+sent[-1]+\" <end>\"\r\n",
    "            descriptions[img].append(des)\r\n",
    "    return descriptions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "train_img_desc = load_cleaned_descriptions(description_path,train_images)\r\n",
    "print(list(train_img_desc.items())[:3])\r\n",
    "\r\n",
    "# these are the images which are present in training images but not present in descriptions  \r\n",
    "for key in train_images:\r\n",
    "    if key not in train_img_desc.keys():\r\n",
    "        print(key)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('1000268201_693b08cb0e.jpg', ['<start> child in pink dress is climbing up set of stairs in an entry way <end>', '<start> girl going into wooden building <end>', '<start> little girl climbing into wooden playhouse <end>', '<start> little girl climbing the stairs to her playhouse <end>', '<start> little girl in pink dress going into wooden cabin <end>']), ('1001773457_577c3a7d70.jpg', ['<start> black dog and spotted dog are fighting <end>', '<start> black dog and tricolored dog playing with each other on the road <end>', '<start> black dog and white dog with brown spots are staring at each other in the street <end>', '<start> two dogs of different breeds looking at each other on the road <end>', '<start> two dogs on pavement moving toward each other <end>']), ('1002674143_1b742ab4b8.jpg', ['<start> little girl covered in paint sits in front of painted rainbow with her hands in bowl <end>', '<start> little girl is sitting in front of large painted rainbow <end>', '<start> small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it <end>', '<start> there is girl with pigtails sitting in front of rainbow painting <end>', '<start> young girl with pigtails painting outside in the grass <end>'])]\n",
      "2837799692_2f1c50722a.jpg\n",
      "3273625566_2454f1556b.jpg\n",
      "2833582518_074bef3ed6.jpg\n",
      "2924483864_cfdb900a13.jpg\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def load_training_features(path,photos):\r\n",
    "    features = load(open(path, 'rb'))\r\n",
    "    required_features = {k:features[k] for k in photos if k not in [\"2837799692_2f1c50722a.jpg\", \"3273625566_2454f1556b.jpg\",\"2833582518_074bef3ed6.jpg\",\"2924483864_cfdb900a13.jpg\"]}\r\n",
    "    return required_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "p_filepath = \"./features.p\"\r\n",
    "training_features = load_training_features(p_filepath,train_images)\r\n",
    "print(len(training_features))\r\n",
    "list(training_features.items())[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5996\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('2513260012_03d33305cf.jpg',\n",
       " array([[0.        , 0.44815865, 0.        , ..., 0.1775745 , 0.00191514,\n",
       "         0.00869677]], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def dict_to_list(dic):\r\n",
    "    ans = []\r\n",
    "    for i in dic.keys():\r\n",
    "        [ans.append(cap) for cap in dic[i]]\r\n",
    "    return ans\r\n",
    "\r\n",
    "def creat_token(desc):\r\n",
    "    desc_list = dict_to_list(desc)\r\n",
    "    tokenizer = Tokenizer()\r\n",
    "    tokenizer.fit_on_texts(desc_list)\r\n",
    "    return tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "dump(tokenizer, open(\"tokenizer.p\",'wb'))\r\n",
    "tokenizer = creat_token(train_img_desc)\r\n",
    "token_length = len(tokenizer.word_index) + 1\r\n",
    "print(token_length)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7576\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def data_generator(descriptions, features, tokenizer, max_length ):\r\n",
    "    while 1:\r\n",
    "        for key , desc_list in descriptions.items():\r\n",
    "            feature = features[key][0]\r\n",
    "            input_img, input_seq, output_word = creat_sequences(tokenizer , max_length, desc_list, feature)\r\n",
    "            yield ([input_img,input_seq], output_word)\r\n",
    "\r\n",
    "def creat_sequences(tokenizer, max_length,desc_list,feature):\r\n",
    "    x1 , x2 , y = list(), list(), list()\r\n",
    "    for desc in desc_list:\r\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\r\n",
    "        for i in range(1,len(seq)):\r\n",
    "            in_seq ,out_seq = seq[:i], seq[i]\r\n",
    "            in_seq = pad_sequences([in_seq],maxlen=max_length)[0]\r\n",
    "            out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\r\n",
    "            x1.append(feature)\r\n",
    "            x2.append(in_seq)\r\n",
    "            y.append(out_seq)\r\n",
    "    return np.array(x1),np.array(x2),np.array(y)\r\n",
    "   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "[[a, b], c] = next(data_generator(train_img_desc, training_features, tokenizer, max_length))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "a.shape,b.shape,c.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((47, 2048), (47, 32), (47, 8763))"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Input\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "from keras.layers.merge import add\r\n",
    "from tensorflow.keras.models import Model, load_model\r\n",
    "\r\n",
    "# define the captioning model\r\n",
    "def define_model(vocab_size, max_length):\r\n",
    "\r\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\r\n",
    "    inputs1 = Input(shape=(2048,))\r\n",
    "    fe1 = Dropout(0.5)(inputs1)\r\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\r\n",
    "    print(inputs1,\"-----\",fe1,\"------\",fe2)\r\n",
    "\r\n",
    "    # LSTM sequence model\r\n",
    "    inputs2 = Input(shape=(max_length,))\r\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\r\n",
    "    se2 = Dropout(0.5)(se1)\r\n",
    "    se3 = LSTM(256)(se2)\r\n",
    "    \r\n",
    "\r\n",
    "    # Merging both models\r\n",
    "    decoder1 = add([fe2, se3])\r\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\r\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n",
    "\r\n",
    "    # tie it together [image, seq] [word]\r\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
    "\r\n",
    "    # summarize model\r\n",
    "\r\n",
    "    plot_model(model, to_file='./model.png', show_shapes=True)\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "print('Dataset: ', len(train_images))\r\n",
    "print('Descriptions: train=', len(train_img_desc))\r\n",
    "print('Photos: train=', len(training_features))\r\n",
    "print('Vocabulary Size:', vocab_size)\r\n",
    "print('Description Length: ', max_length)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 5996\n",
      "Photos: train= 5996\n",
      "Vocabulary Size: 8763\n",
      "Description Length:  32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "model = define_model(vocab_size, max_length)\r\n",
    "epochs = 10\r\n",
    "steps = len(train_img_desc)\r\n",
    "# making a directory models to save our models\r\n",
    "os.mkdir(\"models\")\r\n",
    "for i in range(epochs):\r\n",
    "    generator = data_generator(train_img_desc, training_features, tokenizer, max_length)\r\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\r\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") ----- KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='dropout/Identity:0', description=\"created by layer 'dropout'\") ------ KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='dense/Relu:0', description=\"created by layer 'dense'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\") ------- KerasTensor(type_spec=TensorSpec(shape=(None, 32, 256), dtype=tf.float32, name=None), name='embedding/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding'\") ------ KerasTensor(type_spec=TensorSpec(shape=(None, 32, 256), dtype=tf.float32, name=None), name='dropout_1/Identity:0', description=\"created by layer 'dropout_1'\") ----- KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='lstm/PartitionedCall:0', description=\"created by layer 'lstm'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 32, 256)      2243328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8763)         2252091     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,611,067\n",
      "Trainable params: 5,611,067\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\UIT\\VScode\\Caption_Generator\\venv_caption\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5996/5996 [==============================] - 1658s 275ms/step - loss: 4.5156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d:\\UIT\\VScode\\Caption_Generator\\venv_caption\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5996/5996 [==============================] - 5214s 870ms/step - loss: 3.6574\n",
      "5996/5996 [==============================] - 5624s 938ms/step - loss: 3.3629\n",
      "5996/5996 [==============================] - 3903s 651ms/step - loss: 3.1870\n",
      "5996/5996 [==============================] - 3922s 654ms/step - loss: 3.0680\n",
      "5996/5996 [==============================] - 4091s 682ms/step - loss: 2.9757\n",
      "5996/5996 [==============================] - 4088s 682ms/step - loss: 2.9062\n",
      "5996/5996 [==============================] - 4192s 699ms/step - loss: 2.8482\n",
      "5996/5996 [==============================] - 4063s 678ms/step - loss: 2.8024\n",
      "5996/5996 [==============================] - 4016s 670ms/step - loss: 2.7672\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv_caption': venv)"
  },
  "interpreter": {
   "hash": "b18f688ae25e71b6051206c31f1b828a6045483784f5aadbbe11a2a7f9d6863d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}